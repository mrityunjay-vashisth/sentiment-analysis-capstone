{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothing Brand Sentiment Analysis - Model Results\n",
    "## VADER Sentiment Classification Performance\n",
    "\n",
    "**Objective:** Evaluate sentiment classification performance on clothing brand customer reviews\n",
    "\n",
    "**Model:** VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "- Rule-based sentiment analysis\n",
    "- Specialized for social media and product reviews\n",
    "- No training required\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VADER results on clothing test set\n",
    "results = pd.read_csv('../outputs/clothing_vader_results.csv')\n",
    "\n",
    "print(f\"üìä Dataset: {len(results):,} clothing reviews\")\n",
    "print(f\"   Columns: {results.columns.tolist()}\")\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(results['label'], results['sentiment_label'])\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    results['label'], results['sentiment_label'], \n",
    "    labels=['negative', 'neutral', 'positive'],\n",
    "    average=None\n",
    ")\n",
    "\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ OVERALL PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n   Accuracy:         {accuracy*100:6.2f}%\")\n",
    "print(f\"   Macro Precision:  {macro_precision*100:6.2f}%\")\n",
    "print(f\"   Macro Recall:     {macro_recall*100:6.2f}%\")\n",
    "print(f\"   Macro F1-Score:   {macro_f1*100:6.2f}%\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT:\\n\")\n",
    "print(classification_report(results['label'], results['sentiment_label'], \n",
    "                          labels=['negative', 'neutral', 'positive'],\n",
    "                          target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class metrics\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "bars1 = ax.bar(x - width, precision * 100, width, label='Precision', \n",
    "              color='#2196f3', edgecolor='black', linewidth=1.2)\n",
    "bars2 = ax.bar(x, recall * 100, width, label='Recall', \n",
    "              color='#4caf50', edgecolor='black', linewidth=1.2)\n",
    "bars3 = ax.bar(x + width, f1 * 100, width, label='F1-Score', \n",
    "              color='#ff9800', edgecolor='black', linewidth=1.2)\n",
    "\n",
    "ax.set_ylabel('Percentage (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Sentiment Class', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics - Clothing Brand Reviews', \n",
    "            fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "               f'{height:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/clothing_per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(results['label'], results['sentiment_label'], \n",
    "                     labels=['negative', 'neutral', 'positive'])\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "           yticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "           linewidths=1.5, linecolor='black', cbar_kws={'label': 'Count'},\n",
    "           ax=axes[0])\n",
    "axes[0].set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix - Raw Counts', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Normalized (percentages)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "           xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "           yticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "           linewidths=1.5, linecolor='black', cbar_kws={'label': 'Proportion'},\n",
    "           vmin=0, vmax=1, ax=axes[1])\n",
    "axes[1].set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix - Normalized', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/clothing_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "print(\"\\nüîç MISCLASSIFICATION ANALYSIS:\\n\")\n",
    "print(\"Key Observations:\")\n",
    "\n",
    "# Diagonal accuracy per class\n",
    "for i, label in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    class_accuracy = cm_normalized[i, i] * 100\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Correctly classified: {class_accuracy:.1f}%\")\n",
    "    print(f\"  Most confused with: \", end=\"\")\n",
    "    \n",
    "    # Find max confusion (excluding diagonal)\n",
    "    conf_row = cm_normalized[i].copy()\n",
    "    conf_row[i] = 0  # Exclude diagonal\n",
    "    max_conf_idx = np.argmax(conf_row)\n",
    "    max_conf_label = ['Negative', 'Neutral', 'Positive'][max_conf_idx]\n",
    "    print(f\"{max_conf_label} ({conf_row[max_conf_idx]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confidence Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if confidence scores exist\n",
    "if 'compound_score' in results.columns:\n",
    "    # Analyze VADER compound scores\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Distribution of compound scores\n",
    "    axes[0, 0].hist(results['compound_score'], bins=50, color='#2196f3', \n",
    "                   edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Neutral threshold')\n",
    "    axes[0, 0].set_xlabel('VADER Compound Score', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0, 0].set_title('Distribution of VADER Compound Scores', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Compound scores by true label\n",
    "    for label, color in zip(['negative', 'neutral', 'positive'], \n",
    "                           ['#ef5350', '#ffa726', '#66bb6a']):\n",
    "        data = results[results['label'] == label]['compound_score']\n",
    "        axes[0, 1].hist(data, bins=30, alpha=0.5, label=label.capitalize(), \n",
    "                       color=color, edgecolor='black')\n",
    "    \n",
    "    axes[0, 1].set_xlabel('VADER Compound Score', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0, 1].set_title('Compound Scores by True Sentiment', fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    bp = axes[1, 0].boxplot([results[results['label'] == s]['compound_score'] \n",
    "                            for s in ['negative', 'neutral', 'positive']],\n",
    "                           labels=['Negative', 'Neutral', 'Positive'],\n",
    "                           patch_artist=True, showfliers=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], ['#ef5350', '#ffa726', '#66bb6a']):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[1, 0].set_ylabel('VADER Compound Score', fontweight='bold')\n",
    "    axes[1, 0].set_title('Score Distribution by True Label (Box Plot)', fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Scatter: True vs Predicted scores\n",
    "    label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "    true_numeric = results['label'].map(label_mapping)\n",
    "    \n",
    "    scatter = axes[1, 1].scatter(true_numeric, results['compound_score'],\n",
    "                                c=true_numeric, cmap='RdYlGn', alpha=0.3, s=10)\n",
    "    axes[1, 1].set_xticks([0, 1, 2])\n",
    "    axes[1, 1].set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    axes[1, 1].set_xlabel('True Label', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('VADER Compound Score', fontweight='bold')\n",
    "    axes[1, 1].set_title('True Label vs VADER Score', fontweight='bold')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/clothing_confidence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Confidence scores not available in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance by Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance across product categories\n",
    "category_performance = []\n",
    "\n",
    "for category in results['topic'].unique():\n",
    "    cat_data = results[results['topic'] == category]\n",
    "    if len(cat_data) > 10:  # Only categories with sufficient samples\n",
    "        cat_acc = accuracy_score(cat_data['label'], cat_data['sentiment_label'])\n",
    "        _, _, cat_f1, _ = precision_recall_fscore_support(\n",
    "            cat_data['label'], cat_data['sentiment_label'], \n",
    "            labels=['negative', 'neutral', 'positive'],\n",
    "            average='macro', zero_division=0\n",
    "        )\n",
    "        category_performance.append({\n",
    "            'Category': category,\n",
    "            'Samples': len(cat_data),\n",
    "            'Accuracy': cat_acc * 100,\n",
    "            'Macro F1': cat_f1 * 100\n",
    "        })\n",
    "\n",
    "perf_df = pd.DataFrame(category_performance).sort_values('Macro F1', ascending=False)\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE BY PRODUCT CATEGORY (Top 15):\\n\")\n",
    "print(perf_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category performance\n",
    "top_15_perf = perf_df.head(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(top_15_perf))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, top_15_perf['Accuracy'], width, \n",
    "              label='Accuracy', color='#2196f3', edgecolor='black', linewidth=1)\n",
    "bars2 = ax.bar(x + width/2, top_15_perf['Macro F1'], width, \n",
    "              label='Macro F1', color='#ff9800', edgecolor='black', linewidth=1)\n",
    "\n",
    "ax.set_ylabel('Performance (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Product Category', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Performance by Product Category (Top 15)', \n",
    "            fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top_15_perf['Category'], rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add sample count annotations\n",
    "for i, (idx, row) in enumerate(top_15_perf.iterrows()):\n",
    "    ax.text(i, 5, f\"n={row['Samples']}\", ha='center', va='bottom', \n",
    "           fontsize=8, rotation=90, color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/clothing_performance_by_category.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis - Sample Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified = results[results['label'] != results['sentiment_label']].copy()\n",
    "\n",
    "print(f\"\\n‚ùå Total Misclassified: {len(misclassified):,} out of {len(results):,} ({len(misclassified)/len(results)*100:.1f}%)\\n\")\n",
    "\n",
    "# Sample misclassifications by type\n",
    "print(\"\\nüìù SAMPLE MISCLASSIFICATIONS:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Negative predicted as Positive (False Positives)\n",
    "false_pos = misclassified[(misclassified['label'] == 'negative') & \n",
    "                         (misclassified['sentiment_label'] == 'positive')]\n",
    "if len(false_pos) > 0:\n",
    "    print(\"\\n1Ô∏è‚É£  Negative reviews predicted as POSITIVE:\")\n",
    "    print(\"-\" * 80)\n",
    "    sample = false_pos.sample(min(3, len(false_pos)))\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"\\nCategory: {row['topic']}\")\n",
    "        print(f\"Review: {row['text'][:200]}...\")\n",
    "        if 'compound_score' in row:\n",
    "            print(f\"VADER Score: {row['compound_score']:.3f}\")\n",
    "\n",
    "# Positive predicted as Negative (False Negatives)\n",
    "false_neg = misclassified[(misclassified['label'] == 'positive') & \n",
    "                         (misclassified['sentiment_label'] == 'negative')]\n",
    "if len(false_neg) > 0:\n",
    "    print(\"\\n\\n2Ô∏è‚É£  Positive reviews predicted as NEGATIVE:\")\n",
    "    print(\"-\" * 80)\n",
    "    sample = false_neg.sample(min(3, len(false_neg)))\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"\\nCategory: {row['topic']}\")\n",
    "        print(f\"Review: {row['text'][:200]}...\")\n",
    "        if 'compound_score' in row:\n",
    "            print(f\"VADER Score: {row['compound_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ KEY FINDINGS - CLOTHING BRAND SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  OVERALL PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Macro F1-Score: {macro_f1*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Total Reviews Analyzed: {len(results):,}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  PER-CLASS INSIGHTS:\")\n",
    "for i, label in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    print(f\"   ‚Ä¢ {label:8s}: F1={f1[i]*100:5.2f}%, Precision={precision[i]*100:5.2f}%, Recall={recall[i]*100:5.2f}%\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  CHALLENGES IDENTIFIED:\")\n",
    "if f1[1] < 0.3:  # Neutral class\n",
    "    print(f\"   ‚ö†Ô∏è  Neutral sentiment difficult to detect (F1={f1[1]*100:.1f}%)\")\n",
    "if cm_normalized[0,2] > 0.2:  # Negative -> Positive\n",
    "    print(f\"   ‚ö†Ô∏è  {cm_normalized[0,2]*100:.1f}% of negative reviews misclassified as positive\")\n",
    "    print(f\"      (May contain mixed sentiment or sarcasm)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  CATEGORY INSIGHTS:\")\n",
    "best_cat = perf_df.iloc[0]\n",
    "worst_cat = perf_df.iloc[-1]\n",
    "print(f\"   ‚úÖ Best: '{best_cat['Category']}' (F1={best_cat['Macro F1']:.1f}%)\")\n",
    "print(f\"   ‚ö†Ô∏è  Worst: '{worst_cat['Category']}' (F1={worst_cat['Macro F1']:.1f}%)\")\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£  RECOMMENDATIONS:\")\n",
    "print(f\"   ‚Ä¢ Fine-tune transformer models for better neutral detection\")\n",
    "print(f\"   ‚Ä¢ Consider category-specific models for diverse product types\")\n",
    "print(f\"   ‚Ä¢ Address class imbalance (77% positive reviews)\")\n",
    "print(f\"   ‚Ä¢ Implement ensemble methods combining VADER + ML models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
